{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74c2d493-d600-41d8-a49a-c45484499bc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43a2c82d-be65-4648-8e03-f9bc7c51bdc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Get Last Run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9be390eb-7db9-4913-a75d-7b3d95a8f298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set a configuration variable for the last ingest time\n",
    "last_ingest_df = spark.sql(\"SELECT last_timestamp FROM control.ctl.control_dates WHERE stage_name = 'silver_transformation'\")\n",
    "last_ingest_time = last_ingest_df.collect()[0]['last_timestamp']\n",
    "spark.conf.set(\"last_ingest_time\", str(last_ingest_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cc56d32-107c-476f-ae60-4cccd7d440f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d98b6c6-8e67-4cbd-aa86-e3b13ce59f56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG bronze;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cdc6812-376e-40cb-a958-2ebfc5ee609c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The 2 bronze tables we want to clean/normalise\n",
    "# Compare to configuration variable to only process newly received data\n",
    "df_contacted = spark.read.table(\"customers.contacted\")\\\n",
    "    .filter(col(\"ingest_timestamp\") > spark.conf.get(\"last_ingest_time\"))\\\n",
    "    .drop(\"_rescued_data\")\\\n",
    "    .withColumn(\"ingest_timestamp\", date_format(col(\"ingest_timestamp\"), \"yyyy-MM-dd HH:mm:ss\"))    \n",
    "df_contact_details = spark.read.table(\"customers.contact_details\")\\\n",
    "    .filter(col(\"ingest_timestamp\") > spark.conf.get(\"last_ingest_time\"))\\\n",
    "    .drop(\"_rescued_data\")\\\n",
    "    .withColumn(\"ingest_timestamp\", date_format(col(\"ingest_timestamp\"), \"yyyy-MM-dd HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "302e9aaf-c5bb-4e01-8a06-9141c2812a16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleaning Data\n",
    "\n",
    "Normalise into:\n",
    "- customer\n",
    "- customer_email\n",
    "- customer_address\n",
    "- contact_type_lookup\n",
    "- contact_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4fe93ce-8941-4b42-bc2d-505924a2ffcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The base customer table will be the reference for other customer details\n",
    "df_customer = df_contact_details.select(\"customer_id\", \"first_name\", \"last_name\", \"ingest_timestamp\")\n",
    "\n",
    "# All the normalised tables should only have 1 reference to each customer_id\n",
    "# Order this by most recently received data\n",
    "customer_dedupe_window_spec = Window.partitionBy(\"customer_id\").orderBy(desc(\"ingest_timestamp\"))\n",
    "\n",
    "# Base customer\n",
    "df_customer = df_customer\\\n",
    "    .withColumn(\"rn\", row_number().over(customer_dedupe_window_spec))\\\n",
    "    .filter(col(\"rn\") == 1)\\\n",
    "    .drop(\"rn\")\n",
    "\n",
    "# Customer email only needs records for customers who have an email\n",
    "df_customer_email = df_contact_details.select(\"customer_id\", \"email\", \"ingest_timestamp\")\\\n",
    "    .filter(col(\"email\").isNotNull())\\\n",
    "    .withColumnRenamed(\"email\", \"email_address\")\\\n",
    "    .dropDuplicates()\n",
    "\n",
    "# Customer address only needs records for customers who have a valid zip code\n",
    "df_customer_address = df_contact_details.select(\"customer_id\", \"address\", \"city\", \"state\", \"zip_code\", \"ingest_timestamp\")\\\n",
    "    .filter(col(\"zip_code\") != \"INVALID\")\\\n",
    "    .dropDuplicates()\n",
    "\n",
    "# Dedupe\n",
    "df_customer_email = df_customer_email\\\n",
    "    .withColumn(\"rn\", row_number().over(customer_dedupe_window_spec))\\\n",
    "    .filter(col(\"rn\") == 1)\\\n",
    "    .drop(\"rn\")\n",
    "\n",
    "df_customer_details = df_customer_details\\\n",
    "    .withColumn(\"rn\", row_number().over(customer_dedupe_window_spec))\\\n",
    "    .filter(col(\"rn\") == 1)\\\n",
    "    .drop(\"rn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "299eceac-93d1-4332-8685-283dc202fb8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Contact type is a lookup table\n",
    "df_contact_type = df_contacted.select(\"contact_type\", \"ingest_timestamp\")\n",
    "\n",
    "# Only need 1 record for each contact type\n",
    "df_contact_type = df_contact_type\\\n",
    "    .withColumn(\"rn\", row_number().over(Window.partitionBy(\"contact_type\").orderBy(lit(1))))\\\n",
    "    .filter(col(\"rn\") == 1)\\\n",
    "    .drop(\"rn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7c167d5-0afa-4bbd-92bc-7e8629ba7aae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transform contact_type_lookup\n",
    "\n",
    "This will allow us to use the contact_type_key column in the contact_history table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c1d3dbd-ae87-4087-9bae-da871a949869",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a dataframe of exising records as a reference to see if we have received new contact types\n",
    "# At the same time, store a reference to whether the table exists (used for merge later)\n",
    "# And find the max key (0 if the table doesn't exist) so we can create keys above that value\n",
    "if spark.catalog.tableExists(\"silver.customers.contact_type_lookup\"):\n",
    "    con_lookup_exists = True\n",
    "    max_con_lookup_key = spark.sql(\"SELECT MAX(contact_type_key) AS max_con_lookup_key FROM silver.customers.contact_type_lookup\").collect()[0]['max_con_lookup_key']\n",
    "    df_existing_lookup_records = spark.sql(\"SELECT contact_type_key, contact_type, date_created, last_updated FROM silver.customers.contact_type_lookup\")\n",
    "else:\n",
    "    con_lookup_exists = False\n",
    "    max_con_lookup_key = 0\n",
    "    df_existing_lookup_records = spark.createDataFrame([], schema = StructType([StructField(\"contact_type_key\", IntegerType()), StructField(\"contact_type\", StringType()), StructField(\"date_created\", TimestampType()), StructField(\"last_updated\", TimestampType())]))\n",
    "\n",
    "# Rename columns to avoid conflicts for join\n",
    "df_existing_lookup_records = df_existing_lookup_records\\\n",
    "    .withColumnRenamed(\"contact_type_key\", \"existing_contact_type_key\")\\\n",
    "    .withColumnRenamed(\"contact_type\", \"existing_contact_type\")\\\n",
    "    .withColumnRenamed(\"date_created\", \"existing_date_created\")\\\n",
    "    .withColumnRenamed(\"last_updated\", \"existing_last_updated\")\n",
    "\n",
    "# Left join to split old and new records\n",
    "# If the left join doesn't have a match, the existing_contact_type_key will be null\n",
    "# If the left join has a match, the existing_contact_type_key will be populated\n",
    "df_joined_lookup = df_contact_type.join(df_existing_lookup_records, on = df_existing_lookup_records.existing_contact_type == df_contact_type.contact_type, how = \"left\")\n",
    "df_existing_lookup_records = df_joined_lookup\\\n",
    "    .filter(col(\"existing_contact_type_key\").isNotNull())\n",
    "df_new_lookup_records = df_joined_lookup\\\n",
    "    .filter(col(\"existing_contact_type_key\").isNull())\n",
    "\n",
    "# Create a Window for the row_number() function\n",
    "# This ensures that any newly created contact_key is unique, incremental, and repeatable in case of failure\n",
    "# contact_type has already been deduped, so row_number() behaviour is predictable\n",
    "con_lookup_window_spec = Window.orderBy(\"contact_type\")\n",
    "\n",
    "# Format new records, ingest_timestamp is both the date_created and last_updated\n",
    "# Create key by using row_number() added on to the max key found earlier\n",
    "# Don't need to keep null fields from the join or ingest_timestamp\n",
    "df_new_lookup_records = df_new_lookup_records\\\n",
    "    .withColumn(\"contact_type_key\", row_number().over(con_lookup_window_spec) + lit(max_con_lookup_key))\\\n",
    "    .withColumn(\"date_created\", col(\"ingest_timestamp\"))\\\n",
    "    .withColumn(\"last_updated\", col(\"ingest_timestamp\"))\\\n",
    "    .drop(\"existing_contact_type_key\", \"existing_contact_type\", \"existing_date_created\", \"existing_last_updated\", \"ingest_timestamp\")\n",
    "\n",
    "# Key already exists so just rename the column back to contact_type_key\n",
    "# date_created should be the same as the existing date_created\n",
    "# ingest_timestamp becomes last_updated\n",
    "# Drop duplicate columns\n",
    "df_existing_lookup_records = df_existing_lookup_records\\\n",
    "    .withColumn(\"last_updated\", col(\"ingest_timestamp\"))\\\n",
    "    .withColumnRenamed(\"existing_contact_type_key\", \"contact_type_key\")\\\n",
    "    .withColumnRenamed(\"existing_date_created\", \"date_created\")\\\n",
    "    .drop(\"existing_contact_type\", \"existing_last_updated\", \"ingest_timestamp\")\n",
    "\n",
    "# Union the old and new records\n",
    "df_lookup_final = df_existing_lookup_records.unionByName(df_new_lookup_records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1a9960f-0fb2-4f8e-8a05-b42ba3848f16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Get contact_type_key into contact_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "369b8388-f8d7-431b-b952-bdca0ccd5357",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Replace contact_type in our contacted dataframe with the new contact_type_key created above\n",
    "df_contacted = df_contacted.join(df_lookup_final, on = df_contacted.contact_type == df_lookup_final.contact_type, how = \"inner\")\\\n",
    "    .select(\"contact_id\", \"customer_id\", \"campaign_id\", \"contact_type_key\", \"contact_date\", \"ingest_timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8427a843-09e1-45f5-b375-f2c9fbd0120a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Same logic as above\n",
    "# Keep reference of if contact_history already exists for later merge\n",
    "# Find max surrogate key\n",
    "# Create dataframe of existing records for finding new vs updated records\n",
    "if spark.catalog.tableExists(\"silver.customers.contact_history\"):\n",
    "    con_history_exists = True\n",
    "    max_con_history_key = spark.sql(\"SELECT MAX(contact_key) AS max_con_history_key FROM silver.customers.contact_history\").collect()[0]['max_con_history_key']\n",
    "    df_existing_contact_history = spark.sql(\"SELECT contact_id, contact_key, date_created, last_updated FROM silver.customers.contact_history\")\n",
    "else:\n",
    "    con_history_exists = False\n",
    "    max_con_history_key = 0\n",
    "    df_existing_contact_history = spark.createDataFrame([], schema = StructType([StructField(\"contact_id\", StringType()), StructField(\"contact_key\", IntegerType()), StructField(\"date_created\", TimestampType()), StructField(\"last_updated\", TimestampType())]))\n",
    "\n",
    "# Rename to avoid join conflicts\n",
    "df_existing_contact_history = df_existing_contact_history\\\n",
    "    .withColumnRenamed(\"contact_key\", \"existing_contact_key\")\\\n",
    "    .withColumnRenamed(\"contact_id\", \"existing_contact_id\")\\\n",
    "    .withColumnRenamed(\"date_created\", \"existing_date_created\")\\\n",
    "    .withColumnRenamed(\"last_updated\", \"existing_last_updated\")\n",
    "\n",
    "# Split new vs updated records\n",
    "df_joined_contact_history = df_contacted.join(df_existing_contact_history, on = df_existing_contact_history.existing_contact_id == df_contacted.contact_id, how = \"left\")\n",
    "df_existing_contact_history = df_joined_contact_history\\\n",
    "    .filter(col(\"existing_contact_key\").isNotNull())\n",
    "df_new_contact_history = df_joined_contact_history\\\n",
    "    .filter(col(\"existing_contact_key\").isNull())\n",
    "\n",
    "con_history_window_spec = Window.orderBy(\"contact_id\")\n",
    "\n",
    "df_new_contact_history = df_new_contact_history\\\n",
    "    .withColumn(\"contact_key\", row_number().over(con_history_window_spec) + lit(max_con_history_key))\\\n",
    "    .withColumn(\"date_created\", col(\"ingest_timestamp\"))\\\n",
    "    .withColumn(\"last_updated\", col(\"ingest_timestamp\"))\\\n",
    "    .drop(\"existing_contact_key\", \"existing_contact_id\", \"existing_date_created\", \"existing_last_updated\", \"ingest_timestamp\")\n",
    "\n",
    "df_existing_contact_history = df_existing_contact_history\\\n",
    "    .withColumn(\"last_updated\", col(\"ingest_timestamp\"))\\\n",
    "    .withColumnRenamed(\"existing_contact_key\", \"contact_key\")\\\n",
    "    .withColumnRenamed(\"existing_date_created\", \"date_created\")\\\n",
    "    .drop(\"existing_contact_id\", \"existing_last_updated\", \"ingest_timestamp\")\n",
    "\n",
    "df_contact_history_final = df_existing_contact_history.unionByName(df_new_contact_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "962f0736-7dff-4d41-a2b1-ecc6f8a841a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transform customer\n",
    "\n",
    "As above, this will allow us to use customer_key in customer_email and customer_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3e6e586-e7d6-41ae-a8c7-0471eb3f4997",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Same logic again\n",
    "if spark.catalog.tableExists(\"silver.customers.customer\"):\n",
    "    cust_table_exists = True\n",
    "    max_cust_key = spark.sql(\"SELECT MAX(customer_key) AS max_cust_key FROM silver.customers.customer\").collect()[0]['max_cust_key']\n",
    "    df_existing_customer = spark.sql(\"SELECT customer_key, customer_id, date_created, last_updated FROM silver.customers.customer\")\n",
    "else:\n",
    "    cust_table_exists = False\n",
    "    max_cust_key = 0\n",
    "    df_existing_customer = spark.createDataFrame([], schema = StructType([StructField(\"customer_key\", IntegerType()), StructField(\"customer_id\", StringType()), StructField(\"date_created\", TimestampType()), StructField(\"last_updated\", TimestampType())]))\n",
    "\n",
    "df_existing_customer = df_existing_customer\\\n",
    "    .withColumnRenamed(\"customer_key\", \"existing_customer_key\")\\\n",
    "    .withColumnRenamed(\"customer_id\", \"existing_customer_id\")\\\n",
    "    .withColumnRenamed(\"date_created\", \"existing_date_created\")\\\n",
    "    .withColumnRenamed(\"last_updated\", \"existing_last_updated\")\n",
    "\n",
    "df_joined_customer = df_customer.join(df_existing_customer, on = df_existing_customer.existing_customer_id == df_customer.customer_id, how = \"left\")\n",
    "df_existing_customer = df_joined_customer\\\n",
    "    .filter(col(\"existing_customer_key\").isNotNull())\n",
    "df_new_customer = df_joined_customer\\\n",
    "    .filter(col(\"existing_customer_key\").isNull())\n",
    "\n",
    "cust_window_spec = Window.orderBy(\"customer_id\")\n",
    "\n",
    "df_new_customer = df_new_customer\\\n",
    "    .withColumn(\"customer_key\", row_number().over(cust_window_spec) + lit(max_cust_key))\\\n",
    "    .withColumn(\"date_created\", col(\"ingest_timestamp\"))\\\n",
    "    .withColumn(\"last_updated\", col(\"ingest_timestamp\"))\\\n",
    "    .drop(\"existing_customer_key\", \"existing_customer_id\", \"existing_date_created\", \"existing_last_updated\", \"ingest_timestamp\")\n",
    "\n",
    "df_existing_customer = df_existing_customer\\\n",
    "    .withColumn(\"last_updated\", col(\"ingest_timestamp\"))\\\n",
    "    .withColumnRenamed(\"existing_customer_key\", \"customer_key\")\\\n",
    "    .withColumnRenamed(\"existing_date_created\", \"date_created\")\\\n",
    "    .drop(\"existing_customer_id\", \"existing_last_updated\", \"ingest_timestamp\")\n",
    "\n",
    "df_customer_final = df_existing_customer.unionByName(df_new_customer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2bc835a-6a0e-4f5f-9ddb-a748ba51c42c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Get customer_key into customer_email and customer_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1aaed7a6-b185-47f9-87bb-21f88ca196a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Replace customer_id with newly created customer_key in our customer_email dataframe\n",
    "df_customer_email = df_customer_email.join(df_customer_final, on = df_customer_email.customer_id == df_customer_final.customer_id, how = \"inner\")\\\n",
    "    .select(df_customer_final.customer_key, df_customer_email.email_address, df_customer_email.ingest_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c81c446a-166c-44aa-9f93-dffc6f7ba34d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Do the same with our customer_address dataframe\n",
    "# Also read from our state_regions lookup to add region info to the address\n",
    "df_state_regions = spark.read.table(\"silver.customers.state_region\")\n",
    "\n",
    "df_customer_address = df_customer_address\\\n",
    "    .join(df_customer_final, on = df_customer_address.customer_id == df_customer_final.customer_id, how = \"inner\")\\\n",
    "    .join(df_state_regions, on = df_customer_address.state == df_state_regions.state, how = \"left\")\\\n",
    "    .select(df_customer_final.customer_key, df_customer_address.address, df_customer_address.city, df_customer_address.state, df_state_regions.region_id, df_customer_address.zip_code, df_customer_address.ingest_timestamp)\\\n",
    "    .withColumn(\"region_id\", when(col(\"region_id\").isNull(), lit(\"UNKNOWN\")).otherwise(col(\"region_id\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4abd8ee-44ce-489f-bc57-efd27298fb25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Merge into silver layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82f4c15d-39e3-4892-8391-7a52fb302579",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Merge into or create silver.customers.contact_type_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35a79d8b-2553-4892-8018-6da1c389e4b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if con_lookup_exists:\n",
    "    dlt_con_lookup = DeltaTable.forName(spark, \"silver.customers.contact_type_lookup\")\n",
    "    dlt_con_lookup.alias(\"t\")\\\n",
    "        .merge(df_lookup_final.alias(\"s\"), \"t.contact_type_key = s.contact_type_key\")\\\n",
    "        .whenMatchedUpdateAll()\\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()\n",
    "else:\n",
    "    df_lookup_final.write.mode(\"overwrite\").saveAsTable(\"silver.customers.contact_type_lookup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d912cbf-e385-4d1a-8a0c-175af1cc4fae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Merge into or create silver.customers.contact_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbcb198c-dc29-4e39-948f-7a1f1a095015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if con_history_exists:\n",
    "    dlt_history = DeltaTable.forName(spark, \"silver.customers.contact_history\")\n",
    "    dlt_history.alias(\"t\")\\\n",
    "        .merge(df_contact_history_final.alias(\"s\"), \"t.contact_key = s.contact_key\")\\\n",
    "        .whenMatchedUpdateAll()\\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()\n",
    "else:\n",
    "    df_contact_history_final.write.mode(\"overwrite\").saveAsTable(\"silver.customers.contact_history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e00ec0f-5dca-4fb5-b211-9a90ca28f5c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Merge into or create silver.customers.customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "305621d5-e908-472c-8656-df1b1c7b0ed4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if cust_table_exists:\n",
    "    dlt_cust = DeltaTable.forName(spark, \"silver.customers.customer\")\n",
    "    dlt_cust.alias(\"t\")\\\n",
    "        .merge(df_customer_final.alias(\"s\"), \"t.customer_key = s.customer_key\")\\\n",
    "        .whenMatchedUpdateAll()\\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()\n",
    "else:\n",
    "    df_customer_final.write.mode(\"overwrite\").saveAsTable(\"silver.customers.customer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1d3a051-148e-4a45-84de-0538d7031926",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Merge into or create silver.customers.customer_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "786bac69-0b63-4266-a1f3-b289b38488df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Email is a bit different to above\n",
    "# No surrogate key was created as each customer should only have 1 email on the system\n",
    "# So we merge using customer_key\n",
    "# We also don't have a boolean variable already to see if the table exists,\n",
    "# so must use the spark.catalog\n",
    "if spark.catalog.tableExists(\"silver.customers.customer_email\"):\n",
    "    dlt_cust_email = DeltaTable.forName(spark, \"silver.customers.customer_email\")\n",
    "    dlt_cust_email.alias(\"t\")\\\n",
    "        .merge(df_customer_email.alias(\"s\"), \"t.customer_key = s.customer_key\")\\\n",
    "        .whenMatchedUpdateAll()\\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()\n",
    "else:\n",
    "    df_customer_email.write.mode(\"overwrite\").saveAsTable(\"silver.customers.customer_email\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a3e4ece-dab2-4e8b-a92f-716168b58262",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Merge into or create silver.customers.customer_address:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "245febeb-3cd5-4237-ad79-6338e13fbb7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Address is the same as email above\n",
    "if spark.catalog.tableExists(\"silver.customers.customer_address\"):\n",
    "    dlt_cust_addr = DeltaTable.forName(spark, \"silver.customers.customer_address\")\n",
    "    dlt_cust_addr.alias(\"t\")\\\n",
    "        .merge(df_customer_address.alias(\"s\"), \"t.customer_key = s.customer_key\")\\\n",
    "        .whenMatchedUpdateAll()\\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()\n",
    "else:\n",
    "    df_customer_address.write.mode(\"overwrite\").saveAsTable(\"silver.customers.customer_address\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6849593536199172,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_Customers",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
