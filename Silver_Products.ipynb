{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94c4bd77-9ac7-4c6b-9fbf-8f4c9215d0c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "371bea45-cb52-47e3-9836-95d8594bc237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Get last run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97f7044b-47da-425e-af75-88667b6060bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "last_ingest_df = spark.sql(\"SELECT last_timestamp FROM control.ctl.control_dates WHERE stage_name = 'silver_transformation'\")\n",
    "last_ingest_time = last_ingest_df.collect()[0]['last_timestamp']\n",
    "spark.conf.set(\"last_ingest_time\", str(last_ingest_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85d67377-7cd1-4274-94d4-d7229b5fe1d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c68439-ec2f-419a-8d17-886973623075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG bronze;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6a8cd46-5873-4ff7-8192-04e0db65cd1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"products.products\").filter(col(\"ingest_timestamp\") > spark.conf.get(\"last_ingest_time\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21e386d4-7f86-4a08-8d92-b20854709396",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e62e0a9a-4d1d-44d2-acf1-232ab4aa1e72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"price\", abs(col(\"price\")))\\\n",
    "    .withColumn(\"category\", when((col(\"category\") == \"\") | col(\"category\").isNull(), \"UNKNOWN\").otherwise(col(\"category\")))\\\n",
    "    .withColumn(\"ingest_timestamp\", date_format('ingest_timestamp', 'yyyy-MM-dd HH:mm:ss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51172329-1a0d-4c2d-bc31-76d11262ff62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Split data for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5e13d7e-3a5d-4c8b-b7c8-93b16c33fd6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_categories = df.select(\"category\", \"ingest_timestamp\").dropDuplicates([\"category\"]).withColumn(\"active_flag\", lit(True))\n",
    "df_product_categories = df.select(\"product_id\", \"category\", \"ingest_timestamp\")\n",
    "df_products = df.select(\"product_id\", \"product_name\", \"price\", \"active_flag\", \"ingest_timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f73ef58f-c6b0-4f2b-8f75-8d14f5b51310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Handle Products Table\n",
    "\n",
    "Left join the newly arrived data to existing data to find new records vs updated records <br>\n",
    "If there is no existing table, create an empty dataframe to simulate the existing data with 0 rows <br>\n",
    "Split new records and updated records to handle separately <br>\n",
    "Union them back together for later merging <br>\n",
    "Dedupe as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae7da465-d1fc-483b-b8c6-ddc13ce7e134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if spark.catalog.tableExists(\"silver.products.products\"):\n",
    "    # Get existing products (only relevant columns)\n",
    "    df_products_existing = spark.sql(\"SELECT product_key, product_id, date_created, last_updated FROM silver.products.products\")\n",
    "    # Find current max value for surrogate key\n",
    "    max_prod_key = spark.sql(\"SELECT MAX(product_key) AS max_prod_key FROM silver.products.products\").collect()[0][\"max_prod_key\"]\n",
    "    # Set boolean to mark table exists for later merge\n",
    "    prod_table_exists = True\n",
    "else:\n",
    "    # Create empty df with relevant columns to match above\n",
    "    df_products_existing = df_products.select(\"product_id\")\\\n",
    "        .withColumn(\"product_key\", lit(None)).withColumn(\"product_key\", col(\"product_key\").cast(IntegerType()))\\\n",
    "        .withColumn(\"date_created\", lit('1900-01-01 00:00:00')).withColumn(\"date_created\", col(\"date_created\").cast(TimestampType()))\\\n",
    "        .withColumn(\"last_updated\", lit('1900-01-01 00:00:00')).withColumn(\"last_updated\", col(\"last_updated\").cast(TimestampType()))\\\n",
    "        .filter(col(\"product_key\").isNotNull())\n",
    "    max_prod_key = 0\n",
    "    prod_table_exists = False\n",
    "\n",
    "# Renaming columns before join to avoid any conflicts\n",
    "df_products_existing = df_products_existing.withColumnRenamed(\"product_key\", \"existing_product_key\")\\\n",
    "    .withColumnRenamed(\"product_id\", \"existing_product_id\")\\\n",
    "    .withColumnRenamed(\"date_created\", \"existing_date_created\")\\\n",
    "    .withColumnRenamed(\"last_updated\", \"existing_last_updated\")\n",
    "\n",
    "# Left join to split new products and existing products\n",
    "df_products_joined = df_products.join(df_products_existing, on=df_products.product_id == df_products_existing.existing_product_id, how=\"left\")\n",
    "df_products_new = df_products_joined.filter(col(\"existing_product_key\").isNull())\n",
    "df_products_existing = df_products_joined.filter(col(\"existing_product_key\").isNotNull())\n",
    "\n",
    "# Existing products already have a product_key, date_created, and last_updated\n",
    "# So we remove the extra product_id column\n",
    "# Then remove the \"existing\" from the column names\n",
    "# And set last_updated to ingest_timestamp as this will be when the record has most recently been updated\n",
    "df_products_existing = df_products_existing.drop(\"existing_product_id\", \"existing_last_updated\")\\\n",
    "    .withColumnRenamed(\"ingest_timestamp\", \"last_updated\")\\\n",
    "    .withColumnRenamed(\"existing_product_key\", \"product_key\")\\\n",
    "    .withColumnRenamed(\"existing_date_created\", \"date_created\")\n",
    "\n",
    "# For new products, we need to assign a product_key\n",
    "# Which we do using row_number() over ordering by product_id\n",
    "# Adding on the max product_key that already exists to ensure uniqueness\n",
    "window_spec_prod = Window.orderBy(\"product_id\")\n",
    "# The new products get their product_key, date_created, and last_updated columns created\n",
    "# Before dropping all the other columns\n",
    "df_products_new = df_products_new\\\n",
    "    .withColumn(\"date_created\", col(\"ingest_timestamp\"))\\\n",
    "    .withColumn(\"last_updated\", col(\"ingest_timestamp\"))\\\n",
    "    .withColumn(\"product_key\", row_number().over(window_spec_prod) + lit(max_prod_key))\\\n",
    "    .drop(\"existing_product_id\", \"existing_last_updated\", \"existing_product_key\", \"existing_date_created\", \"ingest_timestamp\")\n",
    "\n",
    "# Union the new products and existing products to have a single dataset to merge into the silver layer\n",
    "df_products_final = df_products_new.unionByName(df_products_existing)\n",
    "\n",
    "prod_dedupe_w_spec = Window.partitionBy(\"product_id\").orderBy(desc(\"last_updated\"))\n",
    "\n",
    "df_products_final = df_products_final\\\n",
    "    .withColumn(\"rn\", row_number().over(prod_dedupe_w_spec))\\\n",
    "    .filter(col(\"rn\") == 1)\\\n",
    "    .drop(\"rn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a520d320-6a3f-4f9b-b71c-542ac17ffd0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Handle Categories Lookup\n",
    "\n",
    "Same as above, but no deduping necessary as dupes were removed when we split up the dataframes for normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34672ae5-376c-4aec-972f-34dca0ebaafd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if spark.catalog.tableExists(\"silver.products.categories_lookup\"):\n",
    "    # Create df with existing categories\n",
    "    df_categories_existing = spark.sql(\"SELECT category_key, category, date_created, last_updated FROM silver.products.categories\")\n",
    "    max_cat_key = spark.sql(\"SELECT MAX(category_key) AS max_cat_key FROM silver.products.category_lookup\").collect()[0][\"max_cat_key\"]\n",
    "    cat_lookup_exists = True\n",
    "else:\n",
    "    # Create empty df with relevant columns\n",
    "    df_categories_existing = df_categories.select(\"category\")\\\n",
    "        .withColumn(\"category_key\", lit(None)).withColumn(\"category_key\", col(\"category_key\").cast(IntegerType()))\\\n",
    "        .withColumn(\"date_created\", lit('1900-01-01 00:00:00')).withColumn(\"date_created\", col(\"date_created\").cast(TimestampType()))\\\n",
    "        .withColumn(\"last_updated\", lit('1900-01-01 00:00:00')).withColumn(\"last_updated\", col(\"last_updated\").cast(TimestampType()))\\\n",
    "        .filter(col(\"category_key\").isNotNull())\n",
    "    max_cat_key = 0\n",
    "    cat_lookup_exists = False\n",
    "\n",
    "df_categories_existing = df_categories_existing.withColumnRenamed(\"category_key\", \"existing_category_key\")\\\n",
    "    .withColumnRenamed(\"category\", \"existing_category\")\\\n",
    "    .withColumnRenamed(\"date_created\", \"existing_date_created\")\\\n",
    "    .withColumnRenamed(\"last_updated\", \"existing_last_updated\")\n",
    "\n",
    "df_categories_joined = df_categories.join(df_categories_existing, on=df_categories.category == df_categories_existing.existing_category, how=\"left\")\n",
    "df_categories_new = df_categories_joined.filter(col(\"existing_category_key\").isNull())\n",
    "df_categories_existing = df_categories_joined.filter(col(\"existing_category_key\").isNotNull())\n",
    "\n",
    "df_categories_existing = df_categories_existing.drop(\"existing_last_updated\", \"existing_category\")\\\n",
    "    .withColumnRenamed(\"ingest_timestamp\", \"last_updated\")\\\n",
    "    .withColumnRenamed(\"existing_category_key\", \"category_key\")\\\n",
    "    .withColumnRenamed(\"existing_date_created\", \"date_created\")\n",
    "\n",
    "window_spec_cat = Window.orderBy(\"category\")\n",
    "\n",
    "df_categories_new = df_categories_new\\\n",
    "    .withColumn(\"last_updated\", col(\"ingest_timestamp\"))\\\n",
    "    .withColumn(\"date_created\", col(\"ingest_timestamp\"))\\\n",
    "    .withColumn(\"category_key\", row_number().over(window_spec_cat) + lit(max_cat_key))\\\n",
    "    .drop(\"existing_category\", \"existing_category_key\", \"existing_date_created\", \"existing_last_updated\", \"ingest_timestamp\")\n",
    "\n",
    "df_categories_final = df_categories_new.unionByName(df_categories_existing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa60a67f-7b99-4585-b105-b4a40a8007a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Handle joining table\n",
    "\n",
    "Dedupe on product (each product can only have 1 category), preserving the most recently ingested record <br>\n",
    "Use the final datasets created above to get the correct surrogate keys <br>\n",
    "If any surrogate keys already existed, they would have been found above <br>\n",
    "Only need to keep surrogate keys for easy joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da66fd66-5c07-4910-8645-6001ee8d5c09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prod_cat_dedupe_w_spec = Window.partitionBy(\"product_id\").orderBy(desc(\"ingest_timestamp\"))\n",
    "\n",
    "df_product_categories = df_product_categories\\\n",
    "    .withColumn(\"rn\", row_number().over(prod_cat_dedupe_w_spec))\\\n",
    "    .filter(col(\"rn\") == 1)\\\n",
    "    .drop(\"rn\")\n",
    "\n",
    "df_product_categories = df_product_categories.join(df_categories_final, on=df_product_categories.category == df_categories_final.category, how=\"inner\")\\\n",
    "    .join(df_products_final, on=df_product_categories.product_id == df_products_final.product_id, how=\"inner\")\\\n",
    "    .select(df_products_final.product_key, df_categories_final.category_key, df_product_categories.ingest_timestamp)\\\n",
    "    .withColumnRenamed(\"ingest_timestamp\", \"last_updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "099da40e-d20d-4f56-b9e0-f3f65b79120b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Write Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "090e0113-0af2-4cd0-9dcb-0e5ad8f4dc7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if prod_table_exists:\n",
    "    dlt_prod = DeltaTable.forName(spark, \"silver.products.products\")\n",
    "    dlt_prod.alias(\"t\").merge(df_products_final.alias(\"s\"), \"t.product_key = s.product_key\")\\\n",
    "        .whenMatchedUpdateAll()\\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()\n",
    "else:\n",
    "    df_products_final.write.mode(\"overwrite\").saveAsTable(\"silver.products.products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bde53767-06db-4fe9-8ce9-3c32772f479e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if cat_lookup_exists:\n",
    "    # Hold delta table object for existing table\n",
    "    dlt_cat = DeltaTable.forName(spark, \"silver.products.categories_lookup\")\n",
    "    # Merge into existing table\n",
    "    dlt_cat.alias(\"t\").merge(df_categories_final.alias(\"s\"), \"t.category_key = s.category_key\")\\\n",
    "        .whenMatchedUpdateAll()\\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()\n",
    "else:\n",
    "    # Create new table if one didn't already exist\n",
    "    # Schema is already set up to save in correct cloud location\n",
    "    df_categories_final.write.mode(\"overwrite\").saveAsTable(\"silver.products.categories_lookup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df6aeaf2-44c2-4328-9f54-a7000d3e65a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if spark.catalog.tableExists(\"silver.products.product_categories\"):\n",
    "    dlt_prod_cat = DeltaTable.forName(spark, \"silver.products.product_categories\")\n",
    "    dlt_prod_cat.alias(\"t\").merge(df_product_categories.alias(\"s\"), \"t.product_key = s.product_key\")\\\n",
    "        .whenMatchedUpdateAll()\\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()\n",
    "else:\n",
    "    df_product_categories.write.mode(\"overwrite\").saveAsTable(\"silver.products.product_categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33436ea2-cf9c-4722-8702-f10700839242",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Update Categories lookup\n",
    "\n",
    "Make sure each category has up to date active_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eede629-6350-4d86-89ca-37d8b51c6999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "UPDATE silver.products.categories_lookup\n",
    "SET active_flag = CASE\n",
    "    WHEN category_key IN (\n",
    "        SELECT pc.category_key\n",
    "        FROM silver.products.product_categories AS pc\n",
    "        JOIN silver.products.products AS p\n",
    "        ON pc.product_key = p.product_key\n",
    "        WHERE p.active_flag = true\n",
    "    ) THEN true\n",
    "    ELSE false\n",
    "END"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6849593536199075,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_Products",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
