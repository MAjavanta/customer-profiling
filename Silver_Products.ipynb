{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94c4bd77-9ac7-4c6b-9fbf-8f4c9215d0c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "371bea45-cb52-47e3-9836-95d8594bc237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Get last run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97f7044b-47da-425e-af75-88667b6060bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "last_ingest_df = spark.sql(\"SELECT last_timestamp FROM control.ctl.control_dates WHERE stage_name = 'silver_transformation'\")\n",
    "last_ingest_time = last_ingest_df.collect()[0]['last_timestamp']\n",
    "spark.conf.set(\"last_ingest_time\", str(last_ingest_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85d67377-7cd1-4274-94d4-d7229b5fe1d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c68439-ec2f-419a-8d17-886973623075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG bronze;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6a8cd46-5873-4ff7-8192-04e0db65cd1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"products.products\").filter(col(\"ingest_timestamp\") > spark.conf.get(\"last_ingest_time\"))\n",
    "df.limit(100).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e62e0a9a-4d1d-44d2-acf1-232ab4aa1e72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"price\", abs(col(\"price\"))).withColumn(\"category\", when((col(\"category\") == \"\") | col(\"category\").isNull(), \"UNKNOWN\").otherwise(col(\"category\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63ee7fa3-a4e9-4f13-ab7c-6963ef3dba12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Normalize the new records into the same schema as the live tables, include surrogate key, create_date, last_updated <br>\n",
    "Split new records into:\n",
    "- to be updated\n",
    "- new\n",
    "\n",
    "Then:<br>\n",
    "Merge records to be updated <br>\n",
    "Insert records to be updated <br>\n",
    "Mark any categories as inactive if they only have inactive products <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5e13d7e-3a5d-4c8b-b7c8-93b16c33fd6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_categories = df.select(\"category\", \"ingest_timestamp\").distinct().withColumn(\"active_flag\", lit(True))\n",
    "df_product_categories = df.select(\"product_id\", \"category\", \"ingest_timestamp\")\n",
    "df_products = df.select(\"product_id\", \"product_name\", \"price\", \"active_flag\", \"ingest_timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae7da465-d1fc-483b-b8c6-ddc13ce7e134",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753109313605}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if spark.catalog.tableExists(\"silver.products.products\"):\n",
    "    # Get existing products (only relevant columns)\n",
    "    df_products_existing = spark.sql(\"SELECT product_key, product_id, date_created, last_updated FROM silver.products.products\")\n",
    "    # Find current max value for surrogate key\n",
    "    max_prod_key = spark.sql(\"SELECT MAX(product_key) AS max_prod_key FROM silver.products.products\").collect()[0][\"max_prod_key\"]\n",
    "else:\n",
    "    # Create empty df with relevant columns to match above\n",
    "    df_products_existing = df_products.select(\"product_id\")\\\n",
    "        .withColumn(\"product_key\", lit(None)).withColumn(\"product_key\", col(\"product_key\").cast(IntegerType()))\\\n",
    "        .withColumn(\"date_created\", lit('1900-01-01T00:00:00')).withColumn(\"date_created\", col(\"date_created\").cast(TimestampType()))\\\n",
    "        .withColumn(\"last_updated\", lit('1900-01-01T00:00:00')).withColumn(\"last_updated\", col(\"last_updated\").cast(TimestampType()))\\\n",
    "        .filter(col(\"product_key\").isNotNull())\n",
    "    max_prod_key = 0\n",
    "\n",
    "# Renaming columns before join to avoid any conflicts\n",
    "df_products_existing = df_products_existing.withColumnRenamed(\"product_key\", \"existing_product_key\")\\\n",
    "    .withColumnRenamed(\"product_id\", \"existing_product_id\")\\\n",
    "    .withColumnRenamed(\"date_created\", \"existing_date_created\")\\\n",
    "    .withColumnRenamed(\"last_updated\", \"existing_last_updated\")\n",
    "\n",
    "# Left join to split new products and existing products\n",
    "df_products_joined = df_products.join(df_products_existing, on=df_products.product_id == df_products_existing.existing_product_id, how=\"left\")\n",
    "df_products_new = df_products_joined.filter(col(\"existing_product_key\").isNull())\n",
    "df_products_existing = df_products_joined.filter(col(\"existing_product_key\").isNotNull())\n",
    "\n",
    "# Existing products already have a product_key, date_created, and last_updated\n",
    "# So we remove the extra product_id column\n",
    "# Then remove the \"existing\" from the column names\n",
    "# And set last_updated to ingest_timestamp as this will be when the record has most recently been updated\n",
    "df_products_existing = df_products_existing.drop(\"existing_product_id\", \"existing_last_updated\")\\\n",
    "    .withColumnRenamed(\"ingest_timestamp\", \"last_updated\")\\\n",
    "    .withColumnRenamed(\"existing_product_key\", \"product_key\")\\\n",
    "    .withColumnRenamed(\"existing_date_created\", \"date_created\")\n",
    "\n",
    "# For new products, we need to assign a product_key\n",
    "# Which we do using row_number() over ordering by product_id\n",
    "# Adding on the max product_key that already exists to ensure uniqueness\n",
    "window_spec_prod = Window.orderBy(\"product_id\")\n",
    "# The new products get their product_key, date_created, and last_updated columns created\n",
    "# Before dropping all the other columns\n",
    "df_products_new = df_products_new\\\n",
    "    .withColumn(\"date_created\", col(\"ingest_timestamp\"))\\\n",
    "    .withColumn(\"last_updated\", col(\"ingest_timestamp\"))\\\n",
    "    .withColumn(\"product_key\", row_number().over(window_spec_prod) + lit(max_prod_key))\\\n",
    "    .drop(\"existing_product_id\", \"existing_last_updated\", \"existing_product_key\", \"existing_date_created\", \"ingest_timestamp\")\n",
    "\n",
    "df_products_new.display()\n",
    "df_products_existing.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34672ae5-376c-4aec-972f-34dca0ebaafd",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753113767540}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if spark.catalog.tableExists(\"silver.products.categories_lookup\"):\n",
    "    # Create df with existing categories\n",
    "    df_categories_existing = spark.sql(\"SELECT category_key, category, date_created, last_updated FROM silver.products.categories\")\n",
    "    max_cat_key = spark.sql(\"SELECT MAX(category_key) AS max_cat_key FROM silver.products.category_lookup\").collect()[0][\"max_cat_key\"]\n",
    "else:\n",
    "    # Create empty df with relevant columns\n",
    "    df_categories_existing = df_categories.select(\"category\")\\\n",
    "        .withColumn(\"category_key\", lit(None)).withColumn(\"category_key\", col(\"category_key\").cast(IntegerType()))\\\n",
    "        .withColumn(\"date_created\", lit('1900-01-01T00:00:00')).withColumn(\"date_created\", col(\"date_created\").cast(TimestampType()))\\\n",
    "        .withColumn(\"last_updated\", lit('1900-01-01T00:00:00')).withColumn(\"last_updated\", col(\"last_updated\").cast(TimestampType()))\\\n",
    "        .filter(col(\"category_key\").isNotNull())\n",
    "    max_cat_key = 0\n",
    "\n",
    "df_categories_existing = df_categories_existing.withColumnRenamed(\"category_key\", \"existing_category_key\")\\\n",
    "    .withColumnRenamed(\"category\", \"existing_category\")\\\n",
    "    .withColumnRenamed(\"date_created\", \"existing_date_created\")\\\n",
    "    .withColumnRenamed(\"last_updated\", \"existing_last_updated\")\n",
    "\n",
    "df_categories_joined = df_categories.join(df_categories_existing, on=df_categories.category == df_categories_existing.existing_category, how=\"left\")\n",
    "df_categories_new = df_categories_joined.filter(col(\"existing_category_key\").isNull())\n",
    "df_categories_existing = df_categories_joined.filter(col(\"existing_category_key\").isNotNull())\n",
    "\n",
    "df_categories_existing = df_categories_existing.drop(\"existing_last_updated\", \"existing_category\")\\\n",
    "    .withColumnRenamed(\"ingest_timestamp\", \"last_updated\")\\\n",
    "    .withColumnRenamed(\"existing_category_key\", \"category_key\")\\\n",
    "    .withColumnRenamed(\"existing_date_created\", \"date_created\")\n",
    "\n",
    "window_spec_cat = Window.orderBy(\"category\")\n",
    "\n",
    "df_categories_new = df_categories_new\\\n",
    "    .withColumn(\"last_updated\", col(\"ingest_timestamp\"))\\\n",
    "    .withColumn(\"date_created\", col(\"ingest_timestamp\"))\\\n",
    "    .withColumn(\"category_key\", row_number().over(window_spec_cat) + lit(max_cat_key))\\\n",
    "    .drop(\"existing_category\", \"existing_category_key\", \"existing_date_created\", \"existing_last_updated\", \"ingest_timestamp\")\n",
    "\n",
    "df_categories_new.display()\n",
    "df_categories_existing.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5284127425410965,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_Products",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
